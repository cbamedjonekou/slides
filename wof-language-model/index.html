<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>WoF Language Model</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!-- link to MathJax New Delimiters, Inline Mode -->
		<script>
			MathJax = {
				tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)'], ['\\{', '\\}']]
				},
				svg: {
				fontCache: 'global'
				}
			};
		</script>
		<script type="text/javascript" id="MathJax-script" async
			src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
		</script>
	
	
		<!-- link to MathJax LaTex CDN -->
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<!-- Cover Slide and TOC -->
				<section>
					<section><h1>Language Modeling w/ WoF</h1></section>
					<section>
							<div id="Table-of-Contents">
								<h3 id="Contents">Contents</h3>
								<ol>
									<li><a href="#Abstract">Abstract</a></li>
									<li><a href="#Prerequisites">Prerequisites</a>
										<ul>
											<li><a href="#keywords">Guiding the Conversation -- Important Keywords</a></li>
										</ul>
									</li>
									<li><a href="#Introduction">Getting Started</a>
										<ul>
											<li><a href="#Packages">Packages</a></li>
										</ul>
									</li>
									<li><a href="#EDA">Exploring our text</a>
										<ul>
											<li><a href="#text-length">What's the text length - in characters?</a></li>
											<li><a href="#unique-letters">The unique letters in our text</a></li>
											<li><a href="#letter-frequency">Letter Frequency</a>
												<ul>
													<li><a href="#1st-order-approx">$1$-$st$ Order Approximations</a></li>
												</ul>
											</li>
											<li><a href="#2nd-order-approx">$2$-$nd$ Order Approximations</a></li>
											<li><a href="#3rd-order-approx">$3$-$rd$ Order Approximations</a></li>
											<li><a href="#1st-order-word">$1$-$st$ Order Word Approximations</a></li>
											<li><a href="#NLTK">Using NLTK to get Parts of Speech</a></li>
											<li></li>
										</ul>
									</li>
								</ol>
							</div>
					</section>
				</section>

				<!-- Abstract Slide -->
				<section>
					<section data-markdown>
						<textarea data-template>
							<h2 id="Abstract">Abstract</h2>
							<p>
								For this project, we set out to create a model that describes solving Wheel of 
								Fortune Bonus Round Puzzles using NLP methods. Wheel of Fortune is a game show 
								that resembles Hangman - in that you have to guess letters in order to solve 
								puzzles. The goal of the project was to implement the model in the form of a 
								neural network. Python (including the NLTK module) was used to conduct the 
								analysis - and model building. Various web technologies (including HTML, CSS, 
								Javascript, D3) were used to build the visualizations. Ultimately, we were unable to 
								produce a working model due to time constraints. I'll describe the process, why it 
								failed -- and overall, why NLP is difficult.
							</p>

							<h3 id="keywords">Guiding the Conversation -- Important Keywords</h3>
							<p>
								Below are important and relevant keywords that I believe will help shape the conversation. 
								I throw them at you now but I will define (and explain) them as they come up:
							</p>

							```sh
								entropy, n-grams, unigrams, bigrams, trigrams, language models, POS-tagging, ergodic, stationary, Hidden Markov Model
							```
						</textarea>
					</section>
				</section>

				<!-- Prerequisite Slide -->
				<section>
					<section data-markdown>
						<textarea data-template>
								<h2 id="Prerequisites">Prerequisites</h2>

								In order to build our model, we had to complete a few things.
								
								* The first thing was collect our data - Our data was on the web, so we use python -- and the `BeautifulSoup` module -- to scrape our data 
								([HOW TO - found here](http://localhost:4000/blog/2019/09/25/Web-Scraping-Series-Web-Scraping-with-BeautifulSoup)). 
									
								* The next thing was to clean our data - We combined bonus round puzzle data that spanned approx. 28 years (1988-2016)  into a single table, 
								and removed all irrelevant and/or unavailable data. What is left is a "clean" dataset that we began to analyze. The task was accomplished using the `pandas` 
								python module ([HOW TO - found here](http://localhost:4000/blog/2019/09/25/Web-Scraping-Series-Web-Scraping-with-BeautifulSoup)).
						</textarea>
					</section>
				</section>

				<!-- Getting Started Slide -->
				<section>
					<!-- Sub Slide 1 - How to start?  -->
					<section data-markdown>
						<textarea data-template>
							<h2 href="#Introduction">Getting Started</h2>

							<!-- Speaker Notes -->
							<aside class="notes">
								<b>1st Note:</b> So how do we actually get started with exploring text data? 
								Turns out that Claude Shannon - the father of Information Theory, for which all 
								Stat NLP tasks are based - asked a similar question. Through his work - Mathematical 
								Theory of Communications (1948) - we learn how to create a description (or model) of language.
								<br> 
								<b>2nd Note:</b> It's the first thing Shannon did when forming his model.
								<br>
								<b>3rd Note:</b> <i>This is said after listing questions</i> - "We'll answer them as we go along". 
							</aside>
							
							The first thing we should do is count the characters (or words) in the text.
							It turns out there are many questions you can answer about the structure of 
							language simply by developing counts for the characters in the text. 
							The following are some of them:
							
							* Length of the text in terms of word count? In terms of character count?
							* How many different types of words — or letters — appear in the text?
							* The frequency of those unique tokens (words, letters)?
							* The most common letters?
							* The most common words?
						</textarea>
					</section>

					<!-- Sub Slide 2 - Packages  -->
					<section data-markdown>
						<textarea data-template>
							<h2 id= "Packages">Packages</h2>

							List of `python` imports used for this analysis
							
							
							```python
							# Package Imports being used to make things happen
							import numpy as np
							import pandas as pd
							import matplotlib.pyplot as plt
							import seaborn as sns
							import string
							from os import listdir
							from os.path import isfile, join
							
							# Special methods/'magic' functions for the visualizations
							sns.set()
							%matplotlib inline
							```
							<!-- Speaker Notes -->
							<aside class="notes">
								But before we move forward, here's the list of `python` 
								imports used for this analysis.
							</aside>
						</textarea>
					</section>
				</section>

				<!-- Text Exploration Slide -->
				<section>
					<section data-markdown>
						<textarea data-template>
							<h2 id="EDA">Exploring our Text</h2>

							<!-- Speaker Notes -->
							<aside class="notes">
								From here on out we explore our data. We can start by answering the questions
								we posed in the previous slides.  
							</aside>
						</textarea>
					</section>
				</section>

				<!-- Question - Character Length -->
				<section>
					<!-- Sub Slide 1 - We're talking about Character Length? -->
					<section data-markdown>
						<textarea data-template>
							<h3 id="text-length">What's the text length - in characters?</h3>

							The feature we're interested in - the `PUZZLE` column:
							<!-- code -->
							```python
							WoF_DF['PUZZLE'].iloc[:10]
							```
							<!-- output -->
							```python
							0        OPEN YOUR EYES
							1         LIZA MINNELLI
							2       PUT ON THE SPOT
							3           FIRST PRIZE
							4           THE VATICAN
							5    FLYING DOWN TO RIO
							6            POGO STICK
							7         YANKEE DOODLE
							8       FINGER PAINTING
							9            JOE NAMATH
							Name: PUZZLE, dtype: object
							```

							`PUZZLE` column to string conversion - using the `.join()` method: 
							<!-- code -->
							```python
							# Step 1: Create a string of text representing all solved bonus round puzzles.
							corpus = ''.join(WoF_DF['PUZZLE'])
							```

							Using the `len()` function to find the length of our string - Result: 33099 characters:
							<!-- code -->
							```python
							len(corpus)
							```
							<!-- output -->
							```python
							33099
							```

							<!-- Speaker Notes -->
							<aside class="notes">
								<b>1st Note:</b> What's the first question we might want to answer? - <i>"What's the 
								character length of our text?"</i>
								<br>
								<b>2nd Note:</b> So, what we're interested in is the `PUZZLE` column.
								<br>
								<b>3rd Note:</b> Here, are the first 10 elements
								<br>
								<b>4th Note:</b> We're going to have to take the `PUZZLE` column and convert it to a string 
								(sequence of characters) using the `.join()` method for strings. 
								<br>
								<b>5th Note:</b> We can now use the `len()` function to find the length of our string - which is 33099 characters.
							</aside>
						</textarea>
					</section>
				</section>

				<!-- Question - Unique Characters -->
				<section>
					<section data-markdown>
						<textarea data-template>
							<h3 id="unique-letters">What are the unique letters (and symbols) in our text?</h3>
							
							Finding the unique symbols - using `np.unique()`:
							<!-- code -->
							```python
							# Creates an array containing each character in the corpus
							corpus_index = np.array([char for char in corpus])
							
							# Finds/Returns an array of unique values
							corpus_index = np.unique(corpus_index)
							print('Unique Characters:\n\n',corpus_index)
							print('\nAmount of Unique Characters:',corpus_index.size)
							```

							The result of our code:
							<!-- output -->
							```python
							Unique Characters:
							
								[' ' '&' "'" '-' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M'
								'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z']
							
							Amount of Unique Characters: 31
							```
							
							<aside class="notes">
								<b>1st Note:</b> Let's consider the next question - How many different types of letters (or symbols) 
								appear in the text?
								<br>
								<b>2nd Note:</b> We expect this `corpus` to have all 26 letters of the alphabet in varying frequency. 
								However, it's always good to check. First, we'll convert our list into a numpy array 
								and use its `.unique()` method.
								<br>
								<b>3rd Note:</b> Just as we suspected; All 26 letters of the english alphabet appear as unique characters.  
								We also have additional characters that we did not expect. The additional characters are mostly punctuation 
								and whitespace, but we'll keep them for now since we don't know yet the significance of the punctuation/whitespace. 
								Additionally, punctuation/whitespace is a significant part of English, so their value cannot be understated.		
							</aside>
						</textarea>
					</section>
				</section>

				<!-- Question - Letter Frequency -->
				<section>
					<!-- Sub Slide 1: Letter Frequency -->
					<section data-markdown>
						<textarea data-template>
							<h3 id="letter-frequency">Letter Frequency - an important question?</h3>
							<!-- code -->

							Counter using a dictionary:
							
							```python
							# Makes a dictionary: Each character in the corpus index (unique characters in corpus)
							# is a 'key' in the dictionary. Each value is initialized to zero
							letter_counter = dict((character, 0) for character in corpus_index)
							
							# Begins the counting, adding 1 for each key match
							for character in corpus:
								letter_counter[character] += 1
							
							# Prints the result
							print(letter_counter)
							```
							
							The result of our count below:
							
							```python
							{' ': 3171, '&': 18, "'": 41, '-': 65, '?': 1, 'A': 2490, 'B': 1072, 'C': 988, 'D': 1038, 'E': 2258, 'F': 926, 'G': 1308, 'H': 1393, 'I': 2396, 'J': 212, 'K': 722, 'L': 1086, 'M': 656, 'N': 1406, 'O': 2834, 'P': 1033, 'Q': 116, 'R': 1514, 'S': 1097, 'T': 1472, 'U': 1346, 'V': 492, 'W': 767, 'X': 117, 'Y': 926, 'Z': 138}
							```
							  
							<aside class="notes">
								<b>1st Note:</b> Based on our results, looks like the letters `EAIOTR` (in no particular order) 
								appear the most in our corpus.
								<br>
								<b>2nd Note:</b> This matches closely to the true frequency of letters in English text.
								<br>
								<b>3rd Note:</b> Whitespace also appears a lot. This makes sense since in most cases 
								since whitespace indicates the beginning/end of a word.
							</aside>
						</textarea>
					</section>

					<!-- Sub Slide 2: Answering Importance -->
					<section data-markdown>
						<textarea data-template>
							<h3>So, what's so important about letter frequency?</h3> 

							* By answering this question, we start to describe what Shannon calls a unigram.

							<aside class="notes">
								<b>1st Note:</b> So why is this question important?
								<br>
								<b>2nd Note:</b> By answering this question, we start describing what 
								Shannon calls a unigram.
							</aside>
						</textarea>
					</section>
				</section>

				<!-- Unigram - Letter Frequency -->
				<section>
					<!-- Sub Slide 1: Defining Language Model & Unigrams -->
					<section data-markdown>
						<textarea data-template>
							<h2>unigrams?</h2>

							[Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) created language models that could be used to approximate
							English text with the purpose of improving Electrical Communications. By finding the frequency distributed over all symbols - 'aka' 
							their frequency distribution - we come out with a Unigram language model. For unigrams, we consider the letters 
							to be independent from each other. We also call unigrams [First-Order Approximations of English text](http://people.seas.harvard.edu/~jones/cscie129/nu_lectures/lecture2/info%20theory/Info_Theory_5.html).
						</textarea>
					</section>

					<!-- Sub Slide 2:  -->
					<section data-markdown>
						<textarea data-template>
							<h3>Visualizing our Unigram Language Model</h3>

							```python
							# Separating the key/value pairs of our dictionary
							letters = list(letter_counter.keys())
							frequency = list(letter_counter.values())
							
							# Creating a dataframe called WoF_Letter_dist
							wof_unigram_dist = pd.DataFrame.from_dict({'Symbol': letters,'Frequency': frequency})

							# Sorts dataframe before plotting
							wof_unigram_dist = wof_unigram_dist.sort_values(by= ['Frequency'], ascending= False)

							# Exports to CSV
							filepath = '/Users/Chris/Desktop/WoF Final Round Puzzles/WoF_unigram_ordered.csv'
							exportTable(filepath, wof_unigram_dist)

							# Plotting the distribution
							sns.barplot(wof_unigram_dist['Symbol'], wof_unigram_dist['Frequency']/len(corpus))
							```

						</textarea>
					</section>
				</section>
			</div>
		</div>

		<!-- Reveal.js Scripts -->
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true },
					{ src: 'https://d3js.org/d3.v4.min.js' },
					{ src: 'node_modules/reveal.js-d3js/d3js.js' }
				]
			});
		</script>

		<!-- Code Toggle Script -->
		<script>
			function codeToggle() {
				var x = document.getElementById("code-toggle");
				if (x.style.display === "none") {
				x.style.display = "block";
				} else {
				x.style.display = "none";
				}
			}
		</script>


	</body>
</html>
