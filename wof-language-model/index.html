<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>WoF Language Model</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!-- link to MathJax New Delimiters, Inline Mode -->
		<script>
			MathJax = {
				tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)'], ['\\{', '\\}']]
				},
				svg: {
				fontCache: 'global'
				}
			};
		</script>

		<script type="text/javascript" id="MathJax-script" async
			src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
		</script>
	
	
		<!-- link to MathJax LaTex CDN -->
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<!-- Introductions -->
				<section>
					<!-- Cover Slide and TOC -->
					<section>
						<h1>Cracking WoF</h1>
						<h3>christ-brian amedjonekou</h3>
						<h3>MAT 4901-D688</h3>
						<h3>LEC (32925)</h3>
						
					</section>

					<!-- Abstract Slide -->
					<section data-markdown>
						<textarea data-template>
							<h2 id="Abstract">Abstract</h2>
							<p>
								For this project, we set out to model Wheel of Fortune - specifically the WoF's bonus 
								round using NLP. Wheel of Fortune is a game show that resembles Hangman - in that you 
								have to guess letters in order to solve puzzles. The goal of the project was to come 
								up with a model that could guess letters that could solve puzzles optimally - 
								implementation would be in the form of a neural network. Python is the tool of choice to conduct the 
								analysis - and model building. The web stack (HTML, CSS, Javascript, plot.ly) is responsible for
								the visualizations. Unfortunately, we were unable to produce a working model due to time 
								constraints. I'll describe the process, some obstacles faced -- and overall, why NLP is 
								difficult.
							</p>

							<h3 id="keywords">Guiding the Conversation -- Important Keywords</h3>
							<p>
								Below are some relevant keywords that I believe will help shape the conversation. 
								I throw them at you now but they'll be explained as they come up:
							</p>

							```sh
												Language Models, n-grams, Unigrams, Bigrams, Trigrams, POS-Tagging
							```
						</textarea>
					</section>

					<!-- Prerequisite Slide -->
					<section data-markdown>
						<textarea data-template>
							<h2 id="Prerequisites">Prerequisites</h2>

							In order to build our model, we had to complete a few things.
							
							* The first thing was collect our data - Our data was on the web, so we use python -- and the `BeautifulSoup` module -- to scrape our data 
							([HOW TO - found here](http://localhost:4000/blog/2019/09/25/Web-Scraping-Series-Web-Scraping-with-BeautifulSoup)). 
								
							* The next thing was to clean our data - We combined bonus round puzzle data that spanned approx. 28 years (1988-2016)  into a single table, 
							and removed all irrelevant and/or unavailable data. What is left is a "clean" dataset that we began to analyze. The task was accomplished using the `pandas` 
							python module ([HOW TO - found here](http://localhost:4000/blog/2019/09/25/Web-Scraping-Series-Web-Scraping-with-BeautifulSoup)).
						</textarea>
					</section>

					<!-- Getting Started  -->
					<section data-markdown>
						<textarea data-template>
							<h3>How does it work - Wheel of Fortune Bonus Round?</h3>

							The player is given a puzzle to solve and is provided all instances of 
							these six letters - `RSTLNE`. The player provides three more consonants 
							(four if they have the Wild Card) and one more vowel. Once all instances 
							of the guessed letters appear, the player tries to solve the puzzle.

							<h3>What's our goal?</h3>
							
							Our goal is to come up with a model for guessing optimal letters to solve puzzles. 

							<h3>So, where do we start?</h3>
							
							The first thing we could do is get counts of stuff from the text. By doing so,
							we might be able to discern some structure within in text.

							<!-- Speaker Notes -->
							<aside class="notes">
								Answer the questions.
							</aside>
						</textarea>
					</section>
				</section>

				<!-- Text Exploration Slide -->
				<section>
					<!-- Section Slide -->
					<section data-markdown>
						<textarea data-template>
							<h2 id="EDA">Exploring our Text</h2>

							<!-- Speaker Notes -->
							<aside class="notes">

							</aside>
						</textarea>
					</section>

					<!-- Question - Character Length -->
					<section data-markdown>
						<textarea data-template>
							<h3 id="text-length">What's the character length of the text - How Big is it?</h3>

							The feature we're interested in - the `PUZZLE` column:
							<!-- code -->
							```python
							WoF_DF['PUZZLE'].iloc[:10]
							```
							<!-- output -->
							```python
							0        OPEN YOUR EYES
							1         LIZA MINNELLI
							2       PUT ON THE SPOT
							3           FIRST PRIZE
							4           THE VATICAN
							5    FLYING DOWN TO RIO
							6            POGO STICK
							7         YANKEE DOODLE
							8       FINGER PAINTING
							9            JOE NAMATH
							Name: PUZZLE, dtype: object
							```

							`PUZZLE` column to string conversion - using the `.join()` method: 
							<!-- code -->
							```python
							# Step 1: Create a string of text representing all solved bonus round puzzles.
							corpus = ''.join(WoF_DF['PUZZLE'])
							```

							Using the `len()` function to find the length of our string - Result: 33099 characters:
							<!-- code -->
							```python
							len(corpus)
							```
							<!-- output -->
							```python
							33099
							```

							<!-- Speaker Notes -->
							<aside class="notes">
								
							</aside>
						</textarea>
					</section>

					<!-- Question - Unique Characters -->
					<section data-markdown>
						<textarea data-template>
							<h3 id="unique-letters">What unique letters appear in our text - and how many?</h3>
							
							Finds the unique characters - using `np.unique()`:
							<!-- code -->
							```python
							# Creates an array containing each character in the corpus
							corpus_index = np.array([char for char in corpus])
							
							# Finds/Returns an array of unique values and the size
							corpus_index = np.unique(corpus_index)
							print('Unique Characters:\n\n',corpus_index)
							print('\nAmount of Unique Characters:',corpus_index.size)
							```

							The result of our code:
							<!-- output -->
							```python
							Unique Characters:
							
								[' ' '&' "'" '-' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M'
								'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z']
							
							Amount of Unique Characters: 31
							```
							
							<aside class="notes">
								1 - The code finds the unique characters and how many.
								2 - We expected all 26 letters of the alphabet to be used - good to see we're correct.
								<br>
								3 - We also have whitespace and punctuation - which are also important. More on this later.
							</aside>
						</textarea>
					</section>

					<!-- Question - Letter Frequency -->
					<section data-markdown>
						<textarea data-template>
							<h3 id="letter-frequency">What about Letter Frequency? Is it an important question?</h3>

							Counter using a dictionary:
							
							```python
							# Makes a dictionary: Each character in the corpus index (unique characters in corpus)
							# is a 'key' in the dictionary. Each value is initialized to zero
							letter_counter = dict((character, 0) for character in corpus_index)
							
							# Begins the counting, adding 1 for each key match
							for character in corpus:
								letter_counter[character] += 1
							
							# Prints the result
							print(letter_counter)
							```
							
							The result of our count below:
							
							```python
							{' ': 3171, '&': 18, "'": 41, '-': 65, '?': 1, 'A': 2490, 'B': 1072, 'C': 988, 'D': 1038, 'E': 2258, 'F': 926, 'G': 1308, 'H': 1393, 'I': 2396, 'J': 212, 'K': 722, 'L': 1086, 'M': 656, 'N': 1406, 'O': 2834, 'P': 1033, 'Q': 116, 'R': 1514, 'S': 1097, 'T': 1472, 'U': 1346, 'V': 492, 'W': 767, 'X': 117, 'Y': 926, 'Z': 138}
							```
							
							`EATOIN` and `whitespace` are the most frequent characters. 
							This is close to the actual distribution of characters in English text. 
							We'll explain the significance later.

							<aside class="notes">
								We'll hold off on the second question until after this slide.
							</aside>
						</textarea>
					</section>

					<!-- Answering if Letter Frequency is important -->
					<section data-markdown>
						<textarea data-template>
							<h3>Is letter frequency important? ðŸ¤”ðŸ¤” - Yessss!! ðŸ˜¤ðŸ˜¤</h3> 

							* By answering this question, we start to describe what [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) 
							(the math guy who developed this stuff) calls n-grams.

							<aside class="notes">
								By answering this question, we start describing what 
								Claude Shannon calls n-grams.
							</aside>
						</textarea>
					</section>
				</section>

				<!-- N-grams - Letter Frequency -->
				<section>
					<!-- Sub Slide 1: Posing the Question -->
					<section data-markdown>
						<textarea data-template>
							<h2>So, what're $n$-grams?</h2>
						</textarea>
					</section>

					<!-- Sub Slide 2: Defining Language Model & n-grams -->
					<section data-markdown>
						<textarea data-template>
							<h3><a href="http://people.seas.harvard.edu/~jones/cscie129/nu_lectures/lecture2/info%20theory/Info_Theory_5.html">Defining: $n$th - Order Approximations of English Text</a></h3>
							
							$n$-grams are language models [Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) created to approximate
							English text -- with the original purpose of efficient transmission (in Electrical Communication systems)
							and code breaking. 
							
							<h5>Description:</h5>

							* $n$ represents the order of approximation, with $n \geq 0$
							* Each increase in order gets use closer to representing English text
							* Shannon posed the question: given a sequence of letters, 
							what is the likelihood of the next letter?

							We'll cover $n = 1, 2, 3,...$-gram cases (and answer Shannon's question) in the following slides. 
						</textarea>
					</section>

					<!-- Sub Slide 3: Defining Unigrams -->
					<section data-markdown>
						<textarea data-template>
							<h3>Defining: $1 \text{st}$ - Order ($n = 1$) Approx. of English Text</h3>
						
							$1$st - Order Approximations of English Text are also called Unigrams. 
							Unigrams are a specific case of Shannon's language model.
							
							<h5>Properties:</h5>

							* The frequency of the characters in a sequence match the frequency of 
							characters in English Text 
							* The characters in the sequence are independent from each other. The next
							character in the sequence **DOES NOT** depend on the previous characters.
	
						</textarea>
					</section>

					<!-- Sub Slide 4: Visualizing Unigrams -->
					<section>
						<h2>Visualizing Unigrams</h2>

						<iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="https://cbamedjonekou.github.io/slides/wof-language-model/html/unigram_ordered.html" height="400" width="100%"></iframe>

						<br>
						By visualizing Unigrams, we are able to confirm the most frequent characters --
						<code>EATION</code> and <code>whitespace</code>. Is this enough to optimally predict letters? Not Really.
						<aside class="notes">
							Letter Frequency is still not enough to optimally predict letters. We need to consider more.
						</aside>
					</section>

					<!-- Sub Slide 5: Defining Bigrams -->
					<section data-markdown>
						<textarea data-template>
							<h3>Defining: $2 \text{nd}$ - Order ($n = 2$) Approx. of English Text</h3>
						
							$2$nd - Order Approximations of English Text are also called Bigrams. 
							Bigrams are also a specific case of Shannon's language model - this time 
							considering conditional probabilities. 
							
							<h5>Properties:</h5>

							* The frequency of the characters in a sequence match the frequency of 
							characters in English Text, just like $1$-st order approximations (unigrams). 
							* The characters in a sequence are **NOT** independent. The next character in 
							the sequence depends on the previous character: $$P(W_n | W_{n-1})$$	
	
						</textarea>
					</section>

					<!-- Sub Slide 6: Visualizing Bigrams -->
					<section >
						<h2>Visualizing Bigrams</h2>
						
						<iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="https://cbamedjonekou.github.io/slides/wof-language-model/html/bigram_heatmap.html" height="400" width="100%"></iframe>

						<br>
						By visualizing Bigrams, we are able to confirm the most frequent characters --
						<code>IN</code>, <code></code>, <code>whitespace</code>. Is this enough to 
						optimally predict letters? Nope, still not enough.
						<aside class="notes">
							Bigram Frequency is a step up from Unigram Frequency but it's still not enough
							to optimally predict letters. We need to consider even more.
						</aside>
					</section>

					<!-- Sub Slide 7: Defining Trigrams -->
					<section data-markdown>
						<textarea data-template>
							<h3>Defining: $3 \text{nd}$ - Order ($n = 3$) Approx. of English Text</h3>
						
							$3$nd - Order Approximations of English Text are also called Trigrams. 
							Trigrams also consider conditional probabilities - the previous 2 terms, however. 
							
							<h5>Properties:</h5>

							* The frequency of the characters in a sequence match still the frequency of 
							characters in English Text, just like with uni- and bi- grams. 
							* With trigrams, the next character in the sequence depends on the 
							2 previous characters: $$P(W_n | W_{1}, W_{n-1})$$	
	
						</textarea>
					</section>

					<!-- Sub Slide 8: Visualizing Trigrams -->
					<section>
						<h2>Visualizing Trigrams</h2>
						
						<iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="https://cbamedjonekou.github.io/slides/wof-language-model/html/trigram_heatmap.html" height="400" width="100%"></iframe>

						<br>
						By visualizing Trigrams, we are able to confirm the most frequent characters --
						<code>EATION</code>, <code></code>, <code>whitespace</code>. Is this enough to 
						optimally predict letters? I wish I could say `yes`.
						<aside class="notes">
							Trigram Frequency are even more of a step up versus uni- and bigram Frequencies but it's still not enough
							to optimally predict letters. We need to consider even more.
						</aside>
					</section>

					<!-- Sub Slide 7: Revisiting n-grams -->
					<section data-markdown>
						<textarea data-template>
							<h3>Revisiting: $n \text{th}$ - Order Word Approx. of English Text</h3>
						
							Just like with letters, we can apply Shannon's models to words. Applying these
							language models to words can help generate better results versus language models
							applied to characters. An just like with letters, a higher order language model 
							does significantly better than the previous order model. 
	
						</textarea>
					</section>

					<!-- Sub Slide 8: Visualizing Unigrams -->
					<section>
						<h2>Visualizing Unigrams again - (context: Words)</h2>
						
						<iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="https://cbamedjonekou.github.io/slides/wof-language-model/html/word_unigram_ordered.html" height="400" width="100%"></iframe>

						<br>
						Through the visualization, we are able to confirm the most frequent words --
						<code>AND</code>, <code>A</code>, <code>IT</code>. Language models applied
						to words are a big step up, it's still not enough.
						<aside class="notes">
							Same as
						</aside>
					</section>
				</section>

				<!-- POS Tagging -->
				<section>
					<!-- Sub Slide 1: Intro slide -->
					<section data-markdown>
						<textarea data-template>
							<h2>POS Tagging - Grammar</h2>
						</textarea>
					</section>

					<!-- Sub Slide 2: Defining Language Model & Unigrams -->
					<section data-markdown>
						<textarea data-template>
							<h3>What's POS-Tagging? Why POS-Tagging?</h3>

							POS stands for Parts of Speech. The intent of POS Tagging is to markup 
							â€” done either automatically, or manually â€” text in order to make finding 
							structure of the language easier. The tags (or tagging) corresponds to grammar
							rules, and structures. POS Tagging is just another step one takes when analyzing text.
						</textarea>
					</section>

					<!-- Sub slide 3: Obstacles -->
					<section>
						<h3>Obstacles with POS Tagging - NLP in general</h3>

						<iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="https://cbamedjonekou.github.io/slides/wof-language-model/html/POS-tagging-compare.html" height="400" width="100%"></iframe>
						<br>

						One of the major obstacles we faced occurred at this stage, when we attempted to tag the POS on the text.
						We have two cases displayed on the chart - red (where the text is all lowercase), and blue 
						(where we have words that begin with a capital letter). Switching case caused massive differences between
						Parts of Speech designations.  
					</section>

					<!-- Sub slide 4: Reasons for obstacles -->
					<section >
						<h2>Reason's as why this happened</h2>
						
						We assumed the reason for this behavior could ultimately be attributed lack of data. The collection of
						text we worked with were short phrases independent from each other. In the case of language, having very 
						large corpora with long lines of text that held a relationship across multiple sequences.
					</section>
				</section>

				<!-- Finishing things up -->
				<section>
					<!-- Cover slide -->
					<section>
						<h2>Finishing Things Up</h2>
					</section>

					<!-- For the Future -->
					<section data-markdown>
						<textarea data-template>
						<h5>A thought for the future:</h5>
						If we were to continue investigate this problem in the future, a good
						place we'd like to start would be with the following equation: 
						$$\frac{1}{\text{# of words}} \sum_{\text{words}} \frac{\text{# missed letters}}{\text{# letters}}$$

						where the numerator could be the [Hamming Distance](https://en.wikipedia.org/wiki/Hamming_distance)
						or the [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance) - both of which 
						measures the minimum number of substitutions required to change one string into the other. The equation 
						could be the cost function we'd need to implement the model via neural net.
						</textarea>
					</section>
				</section>
			</div>
		</div>

		<!-- Reveal.js Scripts -->
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true },
					{ src: 'https://d3js.org/d3.v4.min.js' },
					{ src: 'plugin/d3js/d3js.js' },
					{ src: 'plugin/menu/menu.js' },
					{ src: 'plugin/math/math.js', async: true }
				]
			});
		</script>		
	</body>
</html>
