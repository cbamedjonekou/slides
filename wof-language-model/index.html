<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>WoF Language Model</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!-- link to MathJax New Delimiters, Inline Mode -->
		<script>
			MathJax = {
				tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)'], ['\\{', '\\}']]
				},
				svg: {
				fontCache: 'global'
				}
			};
		</script>

		<script type="text/javascript" id="MathJax-script" async
			src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
		</script>
	
	
		<!-- link to MathJax LaTex CDN -->
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<!-- Cover Slide and TOC -->
				<section>
					<section><h1>Language Modeling w/ WoF</h1></section>
				</section>

				<!-- Introductions -->
				<section>
					<!-- Abstract Slide -->
					<section data-markdown>
						<textarea data-template>
							<h2 id="Abstract">Abstract</h2>
							<p>
								For this project, we set out to model Wheel of Fortune - specifically the WoF's bonus 
								round using NLP. Wheel of Fortune is a game show that resembles Hangman - in that you 
								have to guess letters in order to solve puzzles. The goal of the project was to come 
								up with a model that could guess letters that could solve puzzles optimally - 
								implementation would be in the form of a neural network. Python was used to conduct the 
								analysis - and model building. The web stack (HTML, CSS, Javascript, SVG) was used to build 
								the visualizations. Unfortunately, we were unable to produce a working model due to time 
								constraints. I'll describe the process, some obstacles faced -- and overall, why NLP is 
								difficult.
							</p>

							<h3 id="keywords">Guiding the Conversation -- Important Keywords</h3>
							<p>
								Below are some relevant keywords that I believe will help shape the conversation. 
								I throw them at you now but they'll be explained as they come up:
							</p>

							```sh
							Language Models, n-grams, Unigrams, Bigrams, Trigrams, Ergodic, Stationary, Entropy, POS-Tagging, Hidden Markov Model
							```
						</textarea>
					</section>

					<!-- Prerequisite Slide -->
					<section data-markdown>
						<textarea data-template>
							<h2 id="Prerequisites">Prerequisites</h2>

							In order to build our model, we had to complete a few things.
							
							* The first thing was collect our data - Our data was on the web, so we use python -- and the `BeautifulSoup` module -- to scrape our data 
							([HOW TO - found here](http://localhost:4000/blog/2019/09/25/Web-Scraping-Series-Web-Scraping-with-BeautifulSoup)). 
								
							* The next thing was to clean our data - We combined bonus round puzzle data that spanned approx. 28 years (1988-2016)  into a single table, 
							and removed all irrelevant and/or unavailable data. What is left is a "clean" dataset that we began to analyze. The task was accomplished using the `pandas` 
							python module ([HOW TO - found here](http://localhost:4000/blog/2019/09/25/Web-Scraping-Series-Web-Scraping-with-BeautifulSoup)).
						</textarea>
					</section>

					<!-- Getting Started  -->
					<section data-markdown>
						<textarea data-template>
							<h3>How does it work - Wheel of Fortune Bonus Round?</h3>

							The player is given a puzzle to solve and is provided all instances of 
							these six letters - `RSTLNE`. The player provides three more consonants 
							(four if they have the Wild Card) and one more vowel. Once all instances 
							of the guessed letters appear, the player tries to solve the puzzle.

							<h3>What's our goal?</h3>
							
							Our goal is to come up with a model for guessing optimal letters to solve puzzles. 

							<h3>So, where do we start?</h3>
							
							The first thing we could do is get counts of stuff from the text. By doing so,
							we might be able to discern some structure within in text.

							<!-- Speaker Notes -->
							<aside class="notes">
								Answer the questions.
							</aside>
						</textarea>
					</section>
				</section>

				<!-- Text Exploration Slide -->
				<section>
					<!-- Section Slide -->
					<section data-markdown>
						<textarea data-template>
							<h2 id="EDA">Exploring our Text</h2>

							<!-- Speaker Notes -->
							<aside class="notes">

							</aside>
						</textarea>
					</section>

					<!-- Question - Character Length -->
					<section data-markdown>
						<textarea data-template>
							<h3 id="text-length">What's the character length of the text - How Big is it?</h3>

							The feature we're interested in - the `PUZZLE` column:
							<!-- code -->
							```python
							WoF_DF['PUZZLE'].iloc[:10]
							```
							<!-- output -->
							```python
							0        OPEN YOUR EYES
							1         LIZA MINNELLI
							2       PUT ON THE SPOT
							3           FIRST PRIZE
							4           THE VATICAN
							5    FLYING DOWN TO RIO
							6            POGO STICK
							7         YANKEE DOODLE
							8       FINGER PAINTING
							9            JOE NAMATH
							Name: PUZZLE, dtype: object
							```

							`PUZZLE` column to string conversion - using the `.join()` method: 
							<!-- code -->
							```python
							# Step 1: Create a string of text representing all solved bonus round puzzles.
							corpus = ''.join(WoF_DF['PUZZLE'])
							```

							Using the `len()` function to find the length of our string - Result: 33099 characters:
							<!-- code -->
							```python
							len(corpus)
							```
							<!-- output -->
							```python
							33099
							```

							<!-- Speaker Notes -->
							<aside class="notes">
								
							</aside>
						</textarea>
					</section>

					<!-- Question - Unique Characters -->
					<section data-markdown>
						<textarea data-template>
							<h3 id="unique-letters">What unique letters appear in our text - and how many?</h3>
							
							Finds the unique characters - using `np.unique()`:
							<!-- code -->
							```python
							# Creates an array containing each character in the corpus
							corpus_index = np.array([char for char in corpus])
							
							# Finds/Returns an array of unique values and the size
							corpus_index = np.unique(corpus_index)
							print('Unique Characters:\n\n',corpus_index)
							print('\nAmount of Unique Characters:',corpus_index.size)
							```

							The result of our code:
							<!-- output -->
							```python
							Unique Characters:
							
								[' ' '&' "'" '-' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M'
								'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z']
							
							Amount of Unique Characters: 31
							```
							
							<aside class="notes">
								1 - The code finds the unique characters and how many.
								2 - We expected all 26 letters of the alphabet to be used - good to see we're correct.
								<br>
								3 - We also have whitespace and punctuation - which are also important. More on this later.
							</aside>
						</textarea>
					</section>

					<!-- Question - Letter Frequency -->
					<section data-markdown>
						<textarea data-template>
							<h3 id="letter-frequency">What about Letter Frequency? Is it an important question?</h3>

							Counter using a dictionary:
							
							```python
							# Makes a dictionary: Each character in the corpus index (unique characters in corpus)
							# is a 'key' in the dictionary. Each value is initialized to zero
							letter_counter = dict((character, 0) for character in corpus_index)
							
							# Begins the counting, adding 1 for each key match
							for character in corpus:
								letter_counter[character] += 1
							
							# Prints the result
							print(letter_counter)
							```
							
							The result of our count below:
							
							```python
							{' ': 3171, '&': 18, "'": 41, '-': 65, '?': 1, 'A': 2490, 'B': 1072, 'C': 988, 'D': 1038, 'E': 2258, 'F': 926, 'G': 1308, 'H': 1393, 'I': 2396, 'J': 212, 'K': 722, 'L': 1086, 'M': 656, 'N': 1406, 'O': 2834, 'P': 1033, 'Q': 116, 'R': 1514, 'S': 1097, 'T': 1472, 'U': 1346, 'V': 492, 'W': 767, 'X': 117, 'Y': 926, 'Z': 138}
							```
							
							`EATOIN` and `whitespace` are the most frequent characters. 
							This is close to the actual distribution of characters in English text. 
							We'll explain the significance later.

							<aside class="notes">
								We'll hold off on the second question until after this slide.
							</aside>
						</textarea>
					</section>

					<!-- Answering if Letter Frequency is important -->
					<section data-markdown>
						<textarea data-template>
							<h3>Is letter frequency important? ðŸ¤”ðŸ¤” - Yessss!! ðŸ˜¤ðŸ˜¤</h3> 

							* By answering this question, we start to describe what [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) 
							(the math guy who developed this stuff) calls n-grams.

							<aside class="notes">
								By answering this question, we start describing what 
								Claude Shannon calls n-grams.
							</aside>
						</textarea>
					</section>
				</section>

				<!-- N-grams - Letter Frequency -->
				<section>
					<!-- Sub Slide 1: Posing the Question -->
					<section data-markdown>
						<textarea data-template>
							<h2>So, what're $n$-grams?</h2>
						</textarea>
					</section>

					<!-- Sub Slide 2: Defining Language Model & n-grams -->
					<section data-markdown>
						<textarea data-template>
							<h3><a href="http://people.seas.harvard.edu/~jones/cscie129/nu_lectures/lecture2/info%20theory/Info_Theory_5.html">Defining: $n$th - Order Approximations of English Text</a></h3>
							
							$n$-grams are language models [Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) created to approximate
							English text -- with the original purpose of efficient transmission (in Electrical Communication systems)
							and code breaking. 
							
							<h5>Description:</h5>

							* $n$ represents the order of approximation, with $n \geq 0$
							* Each increase in order gets use closer to representing English text
							* Shannon posed the question: given a sequence of letters, 
							what is the likelihood of the next letter?

							We'll cover $n = 1, 2, 3,...$-gram cases (and answer Shannon's question) in the following slides. 
						</textarea>
					</section>

					<!-- Sub Slide 3: Defining Unigrams -->
					<section data-markdown>
						<textarea data-template>
							<h3>Defining: $1 \text{st}$ - Order ($n = 1$) Approx. of English Text</h3>
						
							$1$st - Order Approximations of English Text are also called Unigrams. 
							Unigrams are a specific case of Shannon's language model.
							
							<h5>Properties:</h5>

							* The frequency of the characters in a sequence match the frequency of 
							characters in English Text 
							* The characters in the sequence are independent from each other. The next
							character in the sequence **DOES NOT** depend on the previous characters.
	
						</textarea>
					</section>

					<!-- Sub Slide 4: Visualizing Unigrams -->
					<section class="fig-container"
							data-fig-id="fig-barchart"
							data-file="barchartD3.html"
							data-transitions="2">
						<h2>Visualizing Unigrams</h2>

						By visualizing Unigrams, we are able to confirm the most frequent characters --
						<code>EATION</code> and <code>whitespace</code>. Is this enough to optimally predict letters? Not Really.
						<aside class="notes">
							Letter Frequency is still not enough to optimally predict letters. We need to consider more.
						</aside>
					</section>

					<!-- Sub Slide 5: Defining Bigrams -->
					<section data-markdown>
						<textarea data-template>
							<h3>Defining: $2 \text{nd}$ - Order ($n = 2$) Approx. of English Text</h3>
						
							$2$nd - Order Approximations of English Text are also called Bigrams. 
							Bigrams are also a specific case of Shannon's language model - this time 
							considering conditional probabilities. 
							
							<h5>Properties:</h5>

							* The frequency of the characters in a sequence match the frequency of 
							characters in English Text, just like $1$-st order approximations (unigrams). 
							* The characters in a sequence are **NOT** independent. The next character in 
							the sequence depends on the previous character: $$P(W_n | W_{n-1})$$	
	
						</textarea>
					</section>

					<!-- Sub Slide 6: Visualizing Bigrams -->
					<section class="fig-container"
							data-fig-id="fig-barchart"
							data-file="barchartD3.html"
							data-transitions="2">
						<h2>Visualizing Bigrams</h2>

						By visualizing Bigrams, we are able to confirm the most frequent characters --
						<code>EATION</code>, <code></code>, <code>whitespace</code>. Is this enough to 
						optimally predict letters? Nope, still not enough.
						<aside class="notes">
							Bigram Frequency is a step of from Unigram Frequency but it's still not enough
							to optimally predict letters. We need to consider even more.
						</aside>
					</section>

					<!-- Sub Slide 6: Unigram Visualization -->
					<section data-markdown>
						<textarea data-template>
							<h3>Visualizing Unigrams - The Code</h3>

							```python
							# Separating the key/value pairs of our dictionary
							letters = list(letter_counter.keys())
							frequency = list(letter_counter.values())
							
							# Creating a dataframe called WoF_Letter_dist
							wof_unigram_dist = pd.DataFrame.from_dict({'Symbol': letters,'Frequency': frequency})

							# Sorts dataframe before plotting
							wof_unigram_dist = wof_unigram_dist.sort_values(by= ['Frequency'], ascending= False)

							# Exports to CSV
							filepath = '/Users/Chris/Desktop/WoF Final Round Puzzles/WoF_unigram_ordered.csv'
							exportTable(filepath, wof_unigram_dist)

							# Plotting the distribution
							sns.barplot(wof_unigram_dist['Symbol'], wof_unigram_dist['Frequency']/len(corpus))
							```
						</textarea>
					</section>
				</section>

				<!-- Unigram - Letter Frequency -->
				<section>
					<!-- Sub Slide 1: Defining Language Model & Unigrams -->
					<section data-markdown>
						<textarea data-template>
							<h2>unigrams</h2>

							[Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) created language models that could be used to approximate
							English text with the purpose of improving Electrical Communications. By finding the frequency distributed over all symbols - 'aka' 
							their frequency distribution - we come out with a Unigram language model. For unigrams, we consider the letters 
							to be independent from each other. We also call unigrams [First-Order Approximations of English text](http://people.seas.harvard.edu/~jones/cscie129/nu_lectures/lecture2/info%20theory/Info_Theory_5.html).
						</textarea>
					</section>

					<!-- Sub Slide 2: Visualizing Unigrams -->
					<section data-markdown>
						<textarea data-template>
							<h3>Visualizing our Unigram Language Model</h3>

							```python
							# Separating the key/value pairs of our dictionary
							letters = list(letter_counter.keys())
							frequency = list(letter_counter.values())
							
							# Creating a dataframe called WoF_Letter_dist
							wof_unigram_dist = pd.DataFrame.from_dict({'Symbol': letters,'Frequency': frequency})

							# Sorts dataframe before plotting
							wof_unigram_dist = wof_unigram_dist.sort_values(by= ['Frequency'], ascending= False)

							# Exports to CSV
							filepath = '/Users/Chris/Desktop/WoF Final Round Puzzles/WoF_unigram_ordered.csv'
							exportTable(filepath, wof_unigram_dist)

							# Plotting the distribution
							sns.barplot(wof_unigram_dist['Symbol'], wof_unigram_dist['Frequency']/len(corpus))
							```

						</textarea>
					</section>

					<!-- Tester slide 1 -->
					<section data-markdown>
						<script type="text/template">
							<!-- .element: class="fig-container" data-fig-id="fig-barchart" data-file="barchartD3.html"-->
						</script>
					</section>

					<!-- Tester slide 2 -->
					<section class="fig-container"
							data-fig-id="fig-barchart"
							data-file="barchartD3.html"
							data-transitions="2">
						<h2>d3.js plugin for reveal.js</h2>

						Press right arrow to trigger transitions and left arrow to invert transition.
					</section>
				</section>
			</div>
		</div>

		<!-- Reveal.js Scripts -->
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true },
					{ src: 'https://d3js.org/d3.v4.min.js' },
					{ src: 'plugin/d3js/d3js.js' },
					{ src: 'plugin/menu/menu.js' },
					{ src: 'plugin/math/math.js', async: true }
				]
			});
		</script>		
	</body>
</html>
